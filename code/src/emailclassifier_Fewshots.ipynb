{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch transformers sentencepiece pandas numpy langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running LLaMA 2 Locally (CPU)\n",
    "#from llama_cpp import Llama\n",
    "\n",
    "#model_path = \"llama_model\\llama-2-7b-chat.Q4_K_M.gguf\"  # Download a GGUF quantized model\n",
    "#llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'email': 'Sunrise Bank Corporation\\n123 Financial Street, New York, NY 10001\\nDate: 15-Apr-2025\\nATTN: John Doe\\nPhone: +1 (212) 555-7890\\nFax: +1 (212) 555-7891\\nEmail: johndoe@wellsfargo.com\\nRe: ABC Holdings LLC – Principal Repayment\\nDeal CUSIP: 123456A78\\nDeal ISIN: US1234567890\\nFacility CUSIP: 987654B32\\nFacility ISIN: US0987654321\\nLender MEI: LND098765\\nEffective 15-Apr-2025, ABC Holdings LLC has elected to make a principal repayment of $250,000 against the loan amount of $5,000,000.\\nPrevious Global Principal Balance: $4,750,000\\nNew Global Principal Balance: $4,500,000\\nThis loan was effective on 01-Jan-2024 and is scheduled to reprice on 01-Jul-2025.\\nYour share of the repayment is $250,000.\\nWe will remit $250,000 on the effective date. Please note that if:\\n(i) the Borrower has not made such payment;\\n(ii) any payment you receive is in excess of what was paid by the Borrower; or\\n(iii) we notify you that the payment was made in error, you agree to return such payment.\\nFor: Sunrise Bank Corporation\\nABA Number: 111002348\\nAccount No.: XXXXXXXX4720\\nReference: ABC Holdings LLC – Principal Repayment\\nThanks & Regards,\\nJane Smith\\n+1 (646) 555-1234\\nSunrise Bank Corporation', 'expected_output': {'request_type': 'Loan Payment', 'sub_request_type': 'Principal Repayment', 'key_attributes': [\"{'Loan Amount': '$5\", '000', \"000'\", \"'Borrower Name': 'ABC Holdings LLC'\", \"'Repayment Amount': '$250\", \"000'\", \"'Due Date': '15-Apr-2025'\", \"'Lender Name': 'Sunrise Bank Corporation'}\"], 'main_intent': 'To confirm a principal repayment'}}, {'email': 'Dear [Recipient Name],\\n\\nThis email is to inform you about a modification to the interest rate on your ABC Loan Facility.\\n\\nThe previous interest rate of 5.2% will be changed to 5.5%, effective from 01-Apr-2025.\\n\\nPlease ensure your records are updated accordingly.\\n\\nIf you have any questions, please do not hesitate to contact us.', 'expected_output': {'request_type': 'Loan Modification', 'sub_request_type': 'Interest Rate Change', 'key_attributes': [\"{'Previous Interest Rate': '5.2%'\", \"'New Interest Rate': '5.5%'\", \"'Effective Date': '01-Apr-2025'\", \"'Loan Reference': 'ABC Loan Facility'}\"], 'main_intent': 'To notify about a change in interest rate'}}, {'email': '[Sunrise Bank Corporation]\\n123 Finance Street, New York, NY 10001\\nDate: 22-Mar-2025\\nATTN: Robert Brown\\nPhone: (555) 345-6789\\nFax: (555) 765-4321\\nEmail: robert.brown@sunrisebank.com\\nRe: Loan Disbursement to XYZ Enterprises\\nEffective 22-Mar-2025, we have disbursed loan funds to XYZ Enterprises under the following terms:\\nDisbursed Amount: $3,500,000\\nFacility Type: Term Loan A\\nThe funds have been transferred to the designated borrower account as per the agreed terms. If you require further details, please let us know.\\nThanks & Regards,\\nSusan Clark\\nSenior Loan Officer\\nSunrise Bank Corporation\\nsusan.clark@sunrisebank.com', 'expected_output': {'request_type': 'Loan Disbursement', 'sub_request_type': 'Fund Transfer', 'key_attributes': [\"{'Disbursed Amount': '$3\", '500', \"000'\", \"'Borrower Name': 'XYZ Enterprises'\", \"'Facility Type': 'Term Loan A'\", \"'Disbursement Date': '22-Mar-2025'}\"], 'main_intent': 'To confirm the disbursement of loan funds'}}, {'email': 'Dear [Recipient Name],\\n\\nThis is a friendly reminder that your loan installment for the XYZ Holdings $100MM Term Loan is due on 30-Mar-2025.\\n\\nThe outstanding amount due is $750,000.\\n\\nPlease note that a penalty of 2% will be applied for any late payments made after the due date.\\n\\nKindly ensure timely payment to avoid any late fees.\\n\\nIf you have any questions or require further assistance, please contact us', 'expected_output': {'request_type': 'Payment Reminder', 'sub_request_type': 'Loan Installment Due', 'key_attributes': [\"{'Payment Due Date': '30-Mar-2025'\", \"'Outstanding Amount': '$750\", \"000'\", \"'Penalty Clause': '2% late fee after due date'\", \"'Loan Reference': 'XYZ Holdings $100MM Term Loan'}\"], 'main_intent': 'To remind about an upcoming payment'}}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def excel_to_few_shot_examples(excel_file_path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and converts it into the specified few-shot examples format.\n",
    "\n",
    "    Args:\n",
    "        excel_file_path (str): The path to the Excel file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the few-shot examples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(excel_file_path)\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {excel_file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "    few_shot_examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        email = row.get(\"email\")\n",
    "        request_type = row.get(\"request_type\")\n",
    "        sub_request_type = row.get(\"sub_request_type\")\n",
    "        key_attributes_str = row.get(\"key_attributes\")\n",
    "        main_intent = row.get(\"main_intent\")\n",
    "\n",
    "        if pd.isna(email) or pd.isna(request_type) or pd.isna(sub_request_type) or pd.isna(main_intent):\n",
    "            print(f\"Warning: Missing data in row {index+1}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        key_attributes = []\n",
    "        if not pd.isna(key_attributes_str):\n",
    "            key_attributes = [attr.strip() for attr in key_attributes_str.split(\",\")]\n",
    "\n",
    "        example = {\n",
    "            \"email\": email,\n",
    "            \"expected_output\": {\n",
    "                \"request_type\": request_type,\n",
    "                \"sub_request_type\": sub_request_type,\n",
    "                \"key_attributes\": key_attributes,\n",
    "                \"main_intent\": main_intent,\n",
    "            },\n",
    "        }\n",
    "        few_shot_examples.append(example)\n",
    "\n",
    "    return few_shot_examples\n",
    "\n",
    "# Example usage (replace 'your_excel_file.xlsx' with your actual file path):\n",
    "result = excel_to_few_shot_examples('Emails.csv')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are a few examples. Please analyze the provided email text and follow the same pattern as demonstrated in the examples below to generate the output.\n",
      "\n",
      "Email:Sunrise Bank Corporation\n",
      "123 Financial Street, New York, NY 10001\n",
      "Date: 15-Apr-2025\n",
      "ATTN: John Doe\n",
      "Phone: +1 (212) 555-7890\n",
      "Fax: +1 (212) 555-7891\n",
      "Email: johndoe@wellsfargo.com\n",
      "Re: ABC Holdings LLC – Principal Repayment\n",
      "Deal CUSIP: 123456A78\n",
      "Deal ISIN: US1234567890\n",
      "Facility CUSIP: 987654B32\n",
      "Facility ISIN: US0987654321\n",
      "Lender MEI: LND098765\n",
      "Effective 15-Apr-2025, ABC Holdings LLC has elected to make a principal repayment of $250,000 against the loan amount of $5,000,000.\n",
      "Previous Global Principal Balance: $4,750,000\n",
      "New Global Principal Balance: $4,500,000\n",
      "This loan was effective on 01-Jan-2024 and is scheduled to reprice on 01-Jul-2025.\n",
      "Your share of the repayment is $250,000.\n",
      "We will remit $250,000 on the effective date. Please note that if:\n",
      "(i) the Borrower has not made such payment;\n",
      "(ii) any payment you receive is in excess of what was paid by the Borrower; or\n",
      "(iii) we notify you that the payment was made in error, you agree to return such payment.\n",
      "For: Sunrise Bank Corporation\n",
      "ABA Number: 111002348\n",
      "Account No.: XXXXXXXX4720\n",
      "Reference: ABC Holdings LLC – Principal Repayment\n",
      "Thanks & Regards,\n",
      "Jane Smith\n",
      "+1 (646) 555-1234\n",
      "Sunrise Bank CorporationOutput:{'request_type': 'Loan Payment', 'sub_request_type': 'Principal Repayment', 'key_attributes': [\"{'Loan Amount': '$5\", '000', \"000'\", \"'Borrower Name': 'ABC Holdings LLC'\", \"'Repayment Amount': '$250\", \"000'\", \"'Due Date': '15-Apr-2025'\", \"'Lender Name': 'Sunrise Bank Corporation'}\"], 'main_intent': 'To confirm a principal repayment'}\n",
      "Email:Dear [Recipient Name],\n",
      "\n",
      "This email is to inform you about a modification to the interest rate on your ABC Loan Facility.\n",
      "\n",
      "The previous interest rate of 5.2% will be changed to 5.5%, effective from 01-Apr-2025.\n",
      "\n",
      "Please ensure your records are updated accordingly.\n",
      "\n",
      "If you have any questions, please do not hesitate to contact us.Output:{'request_type': 'Loan Modification', 'sub_request_type': 'Interest Rate Change', 'key_attributes': [\"{'Previous Interest Rate': '5.2%'\", \"'New Interest Rate': '5.5%'\", \"'Effective Date': '01-Apr-2025'\", \"'Loan Reference': 'ABC Loan Facility'}\"], 'main_intent': 'To notify about a change in interest rate'}\n",
      "Email:[Sunrise Bank Corporation]\n",
      "123 Finance Street, New York, NY 10001\n",
      "Date: 22-Mar-2025\n",
      "ATTN: Robert Brown\n",
      "Phone: (555) 345-6789\n",
      "Fax: (555) 765-4321\n",
      "Email: robert.brown@sunrisebank.com\n",
      "Re: Loan Disbursement to XYZ Enterprises\n",
      "Effective 22-Mar-2025, we have disbursed loan funds to XYZ Enterprises under the following terms:\n",
      "Disbursed Amount: $3,500,000\n",
      "Facility Type: Term Loan A\n",
      "The funds have been transferred to the designated borrower account as per the agreed terms. If you require further details, please let us know.\n",
      "Thanks & Regards,\n",
      "Susan Clark\n",
      "Senior Loan Officer\n",
      "Sunrise Bank Corporation\n",
      "susan.clark@sunrisebank.comOutput:{'request_type': 'Loan Disbursement', 'sub_request_type': 'Fund Transfer', 'key_attributes': [\"{'Disbursed Amount': '$3\", '500', \"000'\", \"'Borrower Name': 'XYZ Enterprises'\", \"'Facility Type': 'Term Loan A'\", \"'Disbursement Date': '22-Mar-2025'}\"], 'main_intent': 'To confirm the disbursement of loan funds'}\n",
      "Email:Dear [Recipient Name],\n",
      "\n",
      "This is a friendly reminder that your loan installment for the XYZ Holdings $100MM Term Loan is due on 30-Mar-2025.\n",
      "\n",
      "The outstanding amount due is $750,000.\n",
      "\n",
      "Please note that a penalty of 2% will be applied for any late payments made after the due date.\n",
      "\n",
      "Kindly ensure timely payment to avoid any late fees.\n",
      "\n",
      "If you have any questions or require further assistance, please contact usOutput:{'request_type': 'Payment Reminder', 'sub_request_type': 'Loan Installment Due', 'key_attributes': [\"{'Payment Due Date': '30-Mar-2025'\", \"'Outstanding Amount': '$750\", \"000'\", \"'Penalty Clause': '2% late fee after due date'\", \"'Loan Reference': 'XYZ Holdings $100MM Term Loan'}\"], 'main_intent': 'To remind about an upcoming payment'}\n",
      "\n",
      "You are an AI email analyzer for a bank. Categorize the email and extract key details. \n",
      "Your output should be a JSON object with the following keys:\n",
      "\n",
      "- **request_type**: General category (e.g., \"Information Request\", \"Support Request\"). \n",
      "- **sub_request_type**: Specific category (e.g., \"Password Reset\", \"Order Status\"). If not applicable, return \"N/A\". \n",
      "- **key_attributes**: A list of key details (e.g., [\"Order Number: 12345\", \"Account ID: ABC1234\"]). If none, return an empty list. \n",
      "- **main_intent**: A concise summary of why the email was written. \n",
      "\n",
      "Email: \n",
      "```This is my new input email body``` \n",
      "\n",
      "Return JSON only, with no extra text before or after.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_few_shot_prompt(email_text):\n",
    "    few_shot_examples = excel_to_few_shot_examples('Emails.csv')\n",
    "\n",
    "    few_shot_prompt = \"Here are a few examples. Please analyze the provided email text and follow the same pattern as demonstrated in the examples below to generate the output.\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        few_shot_prompt += f\"Email:{example['email']}Output:{example['expected_output']}\\n\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "{few_shot_prompt}\n",
    "You are an AI email analyzer for a bank. Categorize the email and extract key details. \n",
    "Your output should be a JSON object with the following keys:\n",
    "\n",
    "- **request_type**: General category (e.g., \"Information Request\", \"Support Request\"). \n",
    "- **sub_request_type**: Specific category (e.g., \"Password Reset\", \"Order Status\"). If not applicable, return \"N/A\". \n",
    "- **key_attributes**: A list of key details (e.g., [\"Order Number: 12345\", \"Account ID: ABC1234\"]). If none, return an empty list. \n",
    "- **main_intent**: A concise summary of why the email was written. \n",
    "\n",
    "Email: \n",
    "```{email_text}``` \n",
    "\n",
    "Return JSON only, with no extra text before or after.\n",
    "\"\"\"\n",
    "    return final_prompt\n",
    "\n",
    "print(create_few_shot_prompt(\"This is my new input email body\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Prompt variable - Markdown not code\n",
    "prompt = f\"\"\"\n",
    "You are an AI email analyzer for a bank. Categorize the email and extract key details.  \n",
    "Your output should be a JSON object with the following keys:\n",
    "\n",
    "- **request_type**: General category (e.g., \"Information Request\", \"Support Request\").  \n",
    "- **sub_request_type**: Specific category (e.g., \"Password Reset\", \"Order Status\"). If not applicable, return \"N/A\".  \n",
    "- **key_attributes**: A list of key details (e.g., [\"Order Number: 12345\", \"Account ID: ABC1234\"]). If none, return an empty list.  \n",
    "- **main_intent**: A concise summary of why the email was written.  \n",
    "\n",
    "Email:  \n",
    "```{email_text}```  \n",
    "\n",
    "Return JSON only, with no extra text before or after.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from llama_cpp import Llama\n",
    "\n",
    "def categorize_email(email_text, model_path):\n",
    "    \"\"\"\n",
    "    Uses Llama 2 (locally) to categorize an email and extract key details.\n",
    "\n",
    "    Args:\n",
    "        email_text: The text content of the email.\n",
    "        model_path: The path to the locally stored LLaMA 2 model.\n",
    "\n",
    "    Returns:\n",
    "        A JSON-formatted string with the categorized information.\n",
    "    \"\"\"\n",
    "    ## ENABLE THSI FEW SHOTS\n",
    "    #prompt = create_few_shot_prompt(email_text)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI email analyzer for a bank. Categorize the email and extract key details.  \n",
    "    Your output should be a JSON object with the following keys:\n",
    "\n",
    "    - **request_type**: General category (e.g., \"Information Request\", \"Support Request\").  \n",
    "    - **sub_request_type**: Specific category (e.g., \"Password Reset\", \"Order Status\"). If not applicable, return \"N/A\".  \n",
    "    - **key_attributes**: A list of key details (e.g., [\"Order Number: 12345\", \"Account ID: ABC1234\"]). If none, return an empty list.  \n",
    "    - **main_intent**: A concise summary of why the email was written.  \n",
    "\n",
    "    Email:  \n",
    "    ```{email_text}```  \n",
    "\n",
    "    Return JSON only, with no extra text before or after.\n",
    "    \"\"\"\n",
    "    # Load LLaMA 2 locally\n",
    "\n",
    "    ## ENABLE THSI FEW SHOTS\n",
    "    #llm = Llama(model_path=model_path, n_ctx=4096)\n",
    "    llm = Llama(model_path=model_path)\n",
    "    # Generate response\n",
    "    response = llm(prompt, max_tokens=5000)\n",
    "\n",
    "    # Extract the text output\n",
    "    response_text = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Extract JSON using regex\n",
    "    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    \n",
    "    if json_match:\n",
    "        try:\n",
    "            result = json.loads(json_match.group(0))\n",
    "            return json.dumps(result, indent=4)\n",
    "        except json.JSONDecodeError:\n",
    "            return json.dumps({\"error\": \"Failed to parse JSON from the model response.\"}, indent=4)\n",
    "    \n",
    "    return json.dumps({\"error\": \"No JSON response detected from the model.\"}, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama_model/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =  102591.68 ms\n",
      "llama_perf_context_print: prompt eval time =  102590.95 ms /   454 tokens (  225.97 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25056.70 ms /    57 runs   (  439.59 ms per token,     2.27 tokens per second)\n",
      "llama_perf_context_print:       total time =  127670.61 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": \"No JSON response detected from the model.\"\n",
      "}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "email_samples = [\n",
    "    # 1️⃣ Account Issue - Unrecognized Transaction\n",
    "    \"\"\"Subject: Email 5: Credit Limit Increase Notification\n",
    "[Sunrise Bank Corporation]\n",
    "123 Finance Street, New York, NY 10001\n",
    "Date: 05-Apr-2025\n",
    "ATTN: Kevin Harris\n",
    "Phone: (555) 567-8901\n",
    "Fax: (555) 543-2109\n",
    "Email: kevin.harris@sunrisebank.com\n",
    "Re: Credit Limit Increase Notification\n",
    "Effective 05-Apr-2025, the credit limit for your loan has been updated as follows:\n",
    "Previous Commitment Amount: $10,000,000\n",
    "New Commitment Amount: $12,500,000\n",
    "Please update your records accordingly. Should you have any questions or require further clarification, please do not hesitate to contact us.\n",
    "Thanks & Regards,\n",
    "Brian Carter\n",
    "Account Manager\n",
    "Sunrise Bank Corporation\n",
    "brian.carter@sunrisebank.com\"\"\" \n",
    "]\n",
    "\n",
    "# Example usage with all sample emails:\n",
    "model_path = \"llama_model/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "for email in email_samples:\n",
    "    print(categorize_email(email, model_path))\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
